# Automatic Data Ingestion

Este script insere dados fictÃ­cios em duas tabelas PostgreSQL â€” uma de cadastro imobiliÃ¡rio e outra de dados geoespaciais â€” utilizando Python, Faker e psycopg2. Ideal para simular dados em pipelines de dados ou ambientes de teste.

## ðŸ“Œ Funcionalidade

A cada execuÃ§Ã£o, o script:
1. Insere 5 registros na tabela `imobiliario.cadastro_mobiliario`
2. Insere 5 registros relacionados na tabela `geoespacial.lotes`
3. Gera logs de execuÃ§Ã£o e erros no diretÃ³rio `logs/`

## âš™ï¸ Estrutura de Banco Esperada

- **Banco:** `PostgreSQL`
- **Tabelas:**
  - `imobiliario.cadastro_mobiliario(inscricao_tecnica, matricula, area_construida, valor_iptu)`
  - `geoespacial.lotes(inscricao_tecnica, geometria, lat_centro, lng_centro, loteamentos)`

## ðŸ“¦ Requisitos

- Python 3.7+
- PostgreSQL rodando com os bancos e schemas configurados
- Acesso aos bancos `ecidades` e `sigeo` com o mesmo host

## ðŸ› ï¸ InstalaÃ§Ã£o

```bash
# Clone o projeto ou copie os arquivos para o servidor
cd /home/deploy/kafka-druid-pipeline/automatic_data_ingestion
```

# Crie e ative o ambiente virtual
```bash
python3 -m venv venv
source venv/bin/activate
````

# Instale as dependÃªncias
```bash
pip install -r requirements.txt
````

## ðŸ•’ Agendamento com cron (a cada 10 minutos)

Edite o crontab:

```bash
crontab -e
```

Adicione a linha:

```bash
*/10 * * * * /home/deploy/kafka-druid-pipeline/automatic_data_ingestion/venv/bin/python /home/deploy/kafka-druid-pipeline/automatic_data_ingestion/main.py >> /home/deploy/kafka-druid-pipeline/automatic_data_ingestion/logs/cron.log 2>&1
```

## ðŸ“‚ Estrutura de Arquivos

```
automatic_data_ingestion/
â”œâ”€â”€ main.py                # Script principal
â”œâ”€â”€ requirements.txt       # DependÃªncias
â”œâ”€â”€ README                 # InstruÃ§Ãµes de uso
â”œâ”€â”€ venv/                  # Ambiente virtual
â””â”€â”€ logs/
    â”œâ”€â”€ error.log          # Log de erros
    â””â”€â”€ execution.log      # Log de execuÃ§Ãµes
```